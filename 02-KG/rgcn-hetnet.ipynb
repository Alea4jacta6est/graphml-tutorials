{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-relational Link Prediction on Knowledge Graphs\n",
    "By Haoxin Li, on 13 July 2020\n",
    "\n",
    "In the biological world, different types of relation could exist between two entities. For example, a drug/chemical compound can act as a *target, enzyme, carrier* or *transporter* on proteins, forming 4 types of edges. Thus, it would not be ideal to represent these relations using the same edge embeddings. In this demo, we explore [Relational Graph Convolutional Neural Network](https://arxiv.org/pdf/1703.06103.pdf) (RGCN) and apply this achitecture on real world biological dataset, including protein-protein interactions, and drug-protein interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, GraphSAINTRandomWalkSampler, NeighborSampler, GraphSAINTEdgeSampler\n",
    "from torch_geometric.nn import RGCNConv, Node2Vec, FastRGCNConv\n",
    "from torch_geometric.utils import contains_isolated_nodes\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score, average_precision_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.load('data/edge_index.pt')\n",
    "row, col = edge_index # row: first row, col: second row\n",
    "edge_attr = torch.load('data/edge_attr.pt')\n",
    "edge_meta_type = torch.load('data/edge_meta_type.pt')\n",
    "edge_type = torch.load('data/edge_type.pt')\n",
    "x = torch.load('data/x.pt')\n",
    "y = torch.load('data/y.pt')\n",
    "num_nodes = len(y) # total number of nodes in the graph\n",
    "\n",
    "train_mask = torch.load('data/train_mask.pt') # training mask of edges, split randomly 80%\n",
    "val_mask = torch.load('data/val_mask.pt') # validation mask of edges, split randomly 10%\n",
    "test_mask = torch.load('data/test_mask.pt') # test_mask of edges, split randomly 10%\n",
    "\n",
    "num_relations = edge_type.unique().size(0) # total number of edge types in the graph\n",
    "\n",
    "data = Data(edge_attr=edge_attr, edge_index=edge_index, edge_type=edge_type, edge_meta_type=edge_meta_type, \n",
    "            x=x, y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `edge_index` stores all the edges in the dataset in the form of a 2-D tensor. Each column represents an edge formed by two nodes and the number of columns indicate the total number of edges in the dataset. For example, the first column in `edge_index` is [0, 9052], which represents an edge between node 0 and node 9052.\n",
    "- `edge_attr` contains edge attributes calulated using `1.0 / torch_geometric.utils.degree(col, num_nodes)[col]`. This attribute is used for GraphSAINT sampler. Please see [this](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/graph_saint.py) and [this](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html) for reference. \n",
    "- `edge_meta_type` helps to identify the meta edge type of each edge in `edge_index`. Because drug and protein edges are directional, we use edge meta types here to do negative sampling more easily.  There are 3 meta edges. `1` represents edges between a drug and a protein, where drug is the starting node and protein is the ending node. `2` represents edges between proteins and proteins. `3` represents edges between a protein and a drug where protein is the starting node and drug is the ending node.\n",
    "- `edge_type` stores the edge type for each edge in `edge_index`. The meaning of each number is shown in the next cell. See `edge_type_mapping`.\n",
    "- `x` stores the input embeddings/attributes of each node, with dimension of 128. It was learnt separately using [node2vec](https://arxiv.org/pdf/1607.00653.pdf). The main reason to use these embeddings is to decrease the input dimension for each node from 25455 to 128. Naively, one-hot-encoded embeddings are used to represent each node.\n",
    "- `y` stores the node type, where `0` represents a drug and `1` represents a protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_type_mapping = {\n",
    "    0: 'target', \n",
    "    1: 'enzyme', \n",
    "    2: 'carrier', \n",
    "    3: 'transporter', \n",
    "    4: 'ppi', \n",
    "    5: 'target_rev',\n",
    "    6: 'enzyme_rev',\n",
    "    7: 'carrier_rev',\n",
    "    8: 'transporter_rev'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 9 different edge types. The last 4 edge types are the opposites of the first 4 edge types as we want our graph to be un-directional.\n",
    "e.g. Drug A **targets** Protein A is equivalent to Protein A is **targeted** by Drug A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = GraphSAINTRandomWalkSampler(data, batch_size=128, walk_length=16, num_steps=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize [GraphSAINT Random Walk Sampler](https://arxiv.org/pdf/1907.04931.pdf) as it allows us to sample fully-connected sub-graphs for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, num_rels):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.num_rels = num_rels\n",
    "        self.conv1 = FastRGCNConv(\n",
    "            in_dim, h_dim, num_rels)\n",
    "        self.conv2 = FastRGCNConv(\n",
    "            h_dim, out_dim, num_rels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.w_rels = nn.Parameter(torch.Tensor(num_rels, out_dim))\n",
    "        nn.init.xavier_uniform_(self.w_rels,\n",
    "                                gain=nn.init.calculate_gain('relu'))\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        x1 = self.conv1(x, edge_index, edge_type)\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.conv2(x1, edge_index, edge_type)\n",
    "        out = F.log_softmax(x2, dim=1)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "def get_metrics(model, embed, edge_index, edge_type, labels):\n",
    "    probs = DistMult(embed, edge_index, edge_type, model)\n",
    "    loss = F.binary_cross_entropy(probs, labels)\n",
    "\n",
    "    probs = probs.cpu().detach().numpy()\n",
    "    labels = labels.cpu().detach().numpy()\n",
    "\n",
    "    return loss, probs, labels\n",
    "\n",
    "def DistMult(embed, edge_index, edge_type, model):\n",
    "    s = embed[edge_index[0, :]]\n",
    "    o = embed[edge_index[1, :]]\n",
    "    r = model.w_rels[edge_type]\n",
    "    scores = torch.sum(s * r * o, dim=1)\n",
    "    \n",
    "    return torch.sigmoid(scores)\n",
    "\n",
    "\n",
    "def get_link_labels(edge_index_pos_len, edge_index_neg_len):\n",
    "    link_labels = torch.zeros(edge_index_pos_len + edge_index_neg_len).float().to(device)\n",
    "    link_labels[:int(edge_index_pos_len)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "def get_embeddings(data):\n",
    "    data = data.to(device)\n",
    "    x = data.x\n",
    "    edge_index_pos = data.edge_index\n",
    "    edge_type = torch.squeeze(data.edge_type)\n",
    "    embed = model(x, edge_index_pos, edge_type)\n",
    "    \n",
    "    return embed\n",
    "\n",
    "def negative_sample(edge_index, edge_meta_type):\n",
    "    \"\"\"\n",
    "    generate negative samples but keep the node type the same\n",
    "    \"\"\"\n",
    "    edge_index_copy = edge_index.clone()\n",
    "    \n",
    "    # resample ppi, the meta edge type for ppi is 2\n",
    "    ppi = edge_index_copy[0, torch.squeeze(edge_meta_type == 2)]\n",
    "    new_index = torch.randperm(ppi.shape[0])\n",
    "    new_ppi = ppi[new_index]\n",
    "    edge_index_copy[0, torch.squeeze(edge_meta_type == 2)] = new_ppi\n",
    "\n",
    "    #resample dpi, the meta edge type for ppi is 1\n",
    "    dpi = edge_index_copy[0, torch.squeeze(edge_meta_type == 1)]\n",
    "    new_index = torch.randperm(dpi.shape[0])\n",
    "    new_dpi = dpi[new_index]\n",
    "    edge_index_copy[0, torch.squeeze(edge_meta_type == 1)] = new_dpi\n",
    "\n",
    "    #resample dpi_rev, the meta edge type for ppi is 3\n",
    "    dpi_rev = edge_index_copy[0, torch.squeeze(edge_meta_type == 3)]\n",
    "    new_index = torch.randperm(dpi_rev.shape[0])\n",
    "    new_dpi_rev = dpi_rev[new_index]\n",
    "    edge_index_copy[0, torch.squeeze(edge_meta_type == 3)] = new_dpi_rev\n",
    "    \n",
    "    return edge_index_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'in_dim': 128, \n",
    "          'h_dim':64,\n",
    "          'out_dim':64,\n",
    "          'num_rels': num_relations,\n",
    "          'epochs':50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RGCN(params['in_dim'], params['h_dim'], params['out_dim'], params['num_rels']).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct a 2-layer RGCN with hidden dimension of 64 for both node and edge embeddings. We model it as a binary classification task that tries to minimize the loss between real edge labels and fake edge labels geneated from negative sampling. We use RGCN as the encoder for node embeddings and DistMult as the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Note: the data for training is sampled from GraphSaint, whereas the data for validation is the whole graph. Parameters initialization may affect model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, embed):\n",
    "    data = data.to(device)\n",
    "    x = data.x\n",
    "    \n",
    "    edge_index_train_pos = data.edge_index[:, data.train_mask]\n",
    "    edge_type_train = torch.squeeze(data.edge_type[data.train_mask])\n",
    "    \n",
    "    edge_meta_type = data.edge_meta_type[data.train_mask]\n",
    "    edge_index_train_neg = negative_sample(edge_index_train_pos, edge_meta_type)\n",
    "\n",
    "    edge_index_train_total = torch.cat([edge_index_train_pos, edge_index_train_neg], dim=-1)\n",
    "    edge_type_train_total = torch.cat([edge_type_train, edge_type_train[:edge_index_train_neg.size(1)]], dim=-1)\n",
    "\n",
    "\n",
    "    link_labels = get_link_labels(edge_index_train_pos.size(1), edge_index_train_neg.size(1))\n",
    "    loss, probs, labels = get_metrics(model, embed, edge_index_train_total, edge_type_train_total, \n",
    "                                            link_labels)\n",
    "    \n",
    "    auroc = roc_auc_score(labels, probs)\n",
    "    auprc = average_precision_score(labels, probs)\n",
    "    \n",
    "    loss_epoch_train.append(loss.item())\n",
    "    auroc_epoch_train.append(auroc)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation(data, embed, evaluate_rel=False):\n",
    "    data = data.to(device)\n",
    "    x = data.x\n",
    "    \n",
    "    edge_index_val_pos = data.edge_index[:, data.val_mask]\n",
    "    edge_type_val = torch.squeeze(data.edge_type[data.val_mask])\n",
    "    \n",
    "    edge_meta_type = data.edge_meta_type[data.val_mask]\n",
    "    edge_index_val_neg = negative_sample(edge_index_val_pos, edge_meta_type)\n",
    "    \n",
    "    edge_index_val_total = torch.cat([edge_index_val_pos, edge_index_val_neg], dim=-1)\n",
    "    edge_type_val_total = torch.cat([edge_type_val, edge_type_val[:edge_index_val_neg.size(1)]], dim=-1)\n",
    "    \n",
    "    link_labels = get_link_labels(edge_index_val_pos.size(1), edge_index_val_neg.size(1))\n",
    "    loss, probs, labels = get_metrics(model, embed, edge_index_val_total, edge_type_val_total, \n",
    "                                                                link_labels)\n",
    "    auroc = roc_auc_score(labels, probs)\n",
    "    auprc = average_precision_score(labels, probs)\n",
    "    \n",
    "    edge_type_val_total = edge_type_val_total.detach().cpu()\n",
    "    \n",
    "    loss_epoch_val.append(loss.item())\n",
    "    auroc_epoch_val.append(auroc)\n",
    "    \n",
    "    if not evaluate_rel:\n",
    "        return\n",
    "    \n",
    "    for i in range(num_relations):\n",
    "        mask = (edge_type_val_total == i)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        probs_per_rel = probs[mask]\n",
    "        labels_per_rel = labels[mask]\n",
    "        auroc_per_rel = roc_auc_score(labels_per_rel, probs_per_rel)\n",
    "        auroc_edge_type[i].append(auroc_per_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train loss: 1.50 | train auroc: 0.50 |\n",
      "Epoch: 1 | val loss: 1.46 | val auroc: 0.50 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 2 | train loss: 0.96 | train auroc: 0.51 |\n",
      "Epoch: 2 | val loss: 0.96 | val auroc: 0.51 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 3 | train loss: 0.90 | train auroc: 0.53 |\n",
      "Epoch: 3 | val loss: 0.91 | val auroc: 0.53 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 4 | train loss: 0.85 | train auroc: 0.60 |\n",
      "Epoch: 4 | val loss: 0.86 | val auroc: 0.60 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 5 | train loss: 0.75 | train auroc: 0.71 |\n",
      "Epoch: 5 | val loss: 0.75 | val auroc: 0.71 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 6 | train loss: 0.70 | train auroc: 0.75 |\n",
      "Epoch: 6 | val loss: 0.68 | val auroc: 0.75 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 7 | train loss: 0.71 | train auroc: 0.77 |\n",
      "Epoch: 7 | val loss: 0.70 | val auroc: 0.77 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 8 | train loss: 0.69 | train auroc: 0.79 |\n",
      "Epoch: 8 | val loss: 0.70 | val auroc: 0.79 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 9 | train loss: 0.67 | train auroc: 0.81 |\n",
      "Epoch: 9 | val loss: 0.67 | val auroc: 0.81 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 10 | train loss: 0.57 | train auroc: 0.82 |\n",
      "Epoch: 10 | val loss: 0.57 | val auroc: 0.82 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 11 | train loss: 0.50 | train auroc: 0.84 |\n",
      "Epoch: 11 | val loss: 0.50 | val auroc: 0.84 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 12 | train loss: 0.49 | train auroc: 0.84 |\n",
      "Epoch: 12 | val loss: 0.49 | val auroc: 0.84 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 13 | train loss: 0.49 | train auroc: 0.84 |\n",
      "Epoch: 13 | val loss: 0.49 | val auroc: 0.84 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 14 | train loss: 0.48 | train auroc: 0.85 |\n",
      "Epoch: 14 | val loss: 0.48 | val auroc: 0.85 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 15 | train loss: 0.46 | train auroc: 0.86 |\n",
      "Epoch: 15 | val loss: 0.46 | val auroc: 0.86 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 16 | train loss: 0.46 | train auroc: 0.86 |\n",
      "Epoch: 16 | val loss: 0.46 | val auroc: 0.86 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 17 | train loss: 0.46 | train auroc: 0.86 |\n",
      "Epoch: 17 | val loss: 0.46 | val auroc: 0.86 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 18 | train loss: 0.46 | train auroc: 0.87 |\n",
      "Epoch: 18 | val loss: 0.46 | val auroc: 0.87 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 19 | train loss: 0.45 | train auroc: 0.87 |\n",
      "Epoch: 19 | val loss: 0.45 | val auroc: 0.87 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 20 | train loss: 0.45 | train auroc: 0.87 |\n",
      "Epoch: 20 | val loss: 0.45 | val auroc: 0.87 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 21 | train loss: 0.44 | train auroc: 0.87 |\n",
      "Epoch: 21 | val loss: 0.44 | val auroc: 0.87 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 22 | train loss: 0.44 | train auroc: 0.88 |\n",
      "Epoch: 22 | val loss: 0.44 | val auroc: 0.88 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 23 | train loss: 0.44 | train auroc: 0.88 |\n",
      "Epoch: 23 | val loss: 0.43 | val auroc: 0.88 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 24 | train loss: 0.42 | train auroc: 0.88 |\n",
      "Epoch: 24 | val loss: 0.42 | val auroc: 0.88 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 25 | train loss: 0.43 | train auroc: 0.88 |\n",
      "Epoch: 25 | val loss: 0.43 | val auroc: 0.88 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 26 | train loss: 0.43 | train auroc: 0.88 |\n",
      "Epoch: 26 | val loss: 0.43 | val auroc: 0.88 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 27 | train loss: 0.43 | train auroc: 0.88 |\n",
      "Epoch: 27 | val loss: 0.43 | val auroc: 0.88 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 28 | train loss: 0.43 | train auroc: 0.88 |\n",
      "Epoch: 28 | val loss: 0.43 | val auroc: 0.88 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 29 | train loss: 0.42 | train auroc: 0.89 |\n",
      "Epoch: 29 | val loss: 0.42 | val auroc: 0.88 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 30 | train loss: 0.42 | train auroc: 0.89 |\n",
      "Epoch: 30 | val loss: 0.42 | val auroc: 0.88 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 31 | train loss: 0.42 | train auroc: 0.89 |\n",
      "Epoch: 31 | val loss: 0.42 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 32 | train loss: 0.42 | train auroc: 0.89 |\n",
      "Epoch: 32 | val loss: 0.42 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 33 | train loss: 0.42 | train auroc: 0.89 |\n",
      "Epoch: 33 | val loss: 0.41 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 34 | train loss: 0.41 | train auroc: 0.89 |\n",
      "Epoch: 34 | val loss: 0.41 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 35 | train loss: 0.42 | train auroc: 0.89 |\n",
      "Epoch: 35 | val loss: 0.42 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 36 | train loss: 0.41 | train auroc: 0.89 |\n",
      "Epoch: 36 | val loss: 0.41 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 37 | train loss: 0.41 | train auroc: 0.89 |\n",
      "Epoch: 37 | val loss: 0.41 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 38 | train loss: 0.41 | train auroc: 0.89 |\n",
      "Epoch: 38 | val loss: 0.41 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 39 | train loss: 0.41 | train auroc: 0.89 |\n",
      "Epoch: 39 | val loss: 0.41 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 40 | train loss: 0.41 | train auroc: 0.89 |\n",
      "Epoch: 40 | val loss: 0.41 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 41 | train loss: 0.40 | train auroc: 0.89 |\n",
      "Epoch: 41 | val loss: 0.40 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 42 | train loss: 0.40 | train auroc: 0.89 |\n",
      "Epoch: 42 | val loss: 0.40 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 43 | train loss: 0.40 | train auroc: 0.89 |\n",
      "Epoch: 43 | val loss: 0.40 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 44 | train loss: 0.41 | train auroc: 0.89 |\n",
      "Epoch: 44 | val loss: 0.41 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 45 | train loss: 0.40 | train auroc: 0.89 |\n",
      "Epoch: 45 | val loss: 0.40 | val auroc: 0.89 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 46 | train loss: 0.41 | train auroc: 0.90 |\n",
      "Epoch: 46 | val loss: 0.41 | val auroc: 0.90 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 47 | train loss: 0.40 | train auroc: 0.90 |\n",
      "Epoch: 47 | val loss: 0.40 | val auroc: 0.90 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 48 | train loss: 0.40 | train auroc: 0.90 |\n",
      "Epoch: 48 | val loss: 0.40 | val auroc: 0.90 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 49 | train loss: 0.40 | train auroc: 0.90 |\n",
      "Epoch: 49 | val loss: 0.40 | val auroc: 0.90 |\n",
      "----------------------------------------------------------------------------------------------\n",
      "Epoch: 50 | train loss: 0.39 | train auroc: 0.90 |\n",
      "Epoch: 50 | val loss: 0.39 | val auroc: 0.90 |\n",
      "----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss_train_total, loss_val_total = [], []\n",
    "auroc_train_total, auroc_val_total = [], []\n",
    "\n",
    "for epoch in range(0, params['epochs']):\n",
    "    loss_epoch_train, loss_epoch_val = [], []\n",
    "    auroc_epoch_train, auroc_epoch_val = [], []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        embed = get_embeddings(batch)\n",
    "        train(batch, embed)\n",
    "        model.eval()\n",
    "        validation(batch, embed)\n",
    "    \n",
    "    loss_train_total.append(np.mean(loss_epoch_train))\n",
    "    auroc_train_total.append(np.mean(auroc_epoch_train))\n",
    "    loss_val_total.append(np.mean(loss_epoch_val))\n",
    "    auroc_val_total.append(np.mean(auroc_epoch_val))\n",
    "\n",
    "    print('Epoch: {} | train loss: {} | train auroc: {} |'.format(epoch + 1, \n",
    "                                                                  \"%.2f\" % np.mean(loss_epoch_train), \n",
    "                                                                  \"%.2f\" % np.mean(auroc_epoch_train)))\n",
    "    print('Epoch: {} | val loss: {} | val auroc: {} |'.format(epoch + 1, \n",
    "                                                              \"%.2f\" % np.mean(loss_epoch_val), \n",
    "                                                              \"%.2f\" % np.mean(auroc_epoch_val)))\n",
    "    \n",
    "    print('----------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc for relation type target: 0.706\n",
      "auroc for relation type enzyme: 0.872\n",
      "auroc for relation type carrier: 0.959\n",
      "auroc for relation type transporter: 0.949\n",
      "auroc for relation type ppi: 0.899\n",
      "auroc for relation type target_rev: 0.724\n",
      "auroc for relation type enzyme_rev: 0.694\n",
      "auroc for relation type carrier_rev: 0.800\n",
      "auroc for relation type transporter_rev: 0.836\n"
     ]
    }
   ],
   "source": [
    "auroc_edge_type = {rel:[] for rel in range(num_relations)}\n",
    "\n",
    "for batch in data_loader:\n",
    "    embed = get_embeddings(batch)\n",
    "    validation(batch, embed, evaluate_rel=True)\n",
    "\n",
    "for rel, values in auroc_edge_type.items():\n",
    "     print('auroc for relation type {}: {}'.format(edge_type_mapping[rel], \"%.3f\" % np.mean(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
